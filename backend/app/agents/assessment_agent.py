
"""
ðŸ“ ASSESSMENT AGENT - INTELLIGENT EVALUATION & TESTING SYSTEM
Sistema avanzato di valutazione con AI-powered assessment, adaptive testing e skill validation
"""
from typing import List, Dict, Optional, Tuple, Any, Union
import asyncio
from datetime import datetime, timedelta
from dataclasses import dataclass
import json
import numpy as np
from enum import Enum
import random

from .base_agent import BaseAgent
from ..services.llm_service import LLMService
from ..services.analytics import AnalyticsService
from ..models.learning import Question, Assessment, Result, Certification

class QuestionType(Enum):
    """Tipi di domande assessment"""
    MULTIPLE_CHOICE = "multiple_choice"
    TRUE_FALSE = "true_false"
    SHORT_ANSWER = "short_answer"
    ESSAY = "essay"
    CODE_CHALLENGE = "code_challenge"
    PRACTICAL_TASK = "practical_task"
    CASE_STUDY = "case_study"

class DifficultyLevel(Enum):
    """Livelli di difficoltÃ """
    BEGINNER = "beginner"
    INTERMEDIATE = "intermediate"
    ADVANCED = "advanced"
    EXPERT = "expert"

class AssessmentType(Enum):
    """Tipi di assessment"""
    DIAGNOSTIC = "diagnostic"
    FORMATIVE = "formative"
    SUMMATIVE = "summative"
    CERTIFICATION = "certification"
    ADAPTIVE = "adaptive"
    PEER_REVIEW = "peer_review"

@dataclass
class AdaptiveQuestion:
    """Domanda adattiva con metadati"""
    id: str
    question_text: str
    question_type: QuestionType
    difficulty_level: DifficultyLevel
    skill_tags: List[str]
    options: Optional[List[str]]
    correct_answer: Union[str, List[str]]
    explanation: str
    estimated_time_minutes: int
    points: int
    prerequisite_concepts: List[str]
    learning_objectives: List[str]
    cognitive_level: str  # bloom's taxonomy

@dataclass
class AssessmentResult:
    """Risultato assessment completo"""
    user_id: str
    assessment_id: str
    overall_score: float  # 0-1
    skill_scores: Dict[str, float]
    time_taken_minutes: int
    questions_attempted: int
    questions_correct: int
    difficulty_progression: List[DifficultyLevel]
    knowledge_gaps: List[str]
    mastery_areas: List[str]
    recommendations: List[str]
    confidence_intervals: Dict[str, Tuple[float, float]]
    next_learning_steps: List[str]
    certification_eligible: bool
    detailed_feedback: str

@dataclass
class SkillAssessment:
    """Assessment specifico per skill"""
    skill_name: str
    proficiency_level: DifficultyLevel
    confidence_score: float
    evidence_strength: int  # numero di domande/task validate
    sub_skills: Dict[str, float]
    improvement_areas: List[str]
    validation_methods: List[str]
    last_assessed: datetime

class AssessmentAgent(BaseAgent):
    """
    ðŸ“ ASSESSMENT AGENT

    FunzionalitÃ :
    - Adaptive testing con AI
    - Question generation intelligente
    - Multi-modal assessment (quiz, coding, projects)
    - Skill proficiency mapping
    - Automated grading con NLP
    - Peer assessment orchestration
    - Certification pathways
    - Learning analytics e insights
    """

    def __init__(self, llm_service: LLMService, analytics_service: AnalyticsService):
        super().__init__("AssessmentAgent", "Intelligent evaluation and skill assessment")
        self.llm_service = llm_service
        self.analytics_service = analytics_service

        # Assessment configuration
        self.difficulty_thresholds = {
            DifficultyLevel.BEGINNER: (0.0, 0.4),
            DifficultyLevel.INTERMEDIATE: (0.4, 0.7),
            DifficultyLevel.ADVANCED: (0.7, 0.9),
            DifficultyLevel.EXPERT: (0.9, 1.0)
        }

        # Cognitive levels (Bloom's Taxonomy)
        self.cognitive_levels = [
            'remember', 'understand', 'apply', 'analyze', 'evaluate', 'create'
        ]

        # Assessment strategies per skill type
        self.assessment_strategies = {
            'technical': ['code_challenge', 'practical_task', 'multiple_choice'],
            'conceptual': ['short_answer', 'essay', 'case_study'],
            'practical': ['practical_task', 'peer_review', 'portfolio']
        }

    async def create_adaptive_assessment(
        self,
        user_id: str,
        skill_area: str,
        assessment_type: AssessmentType = AssessmentType.ADAPTIVE,
        target_duration_minutes: int = 30
    ) -> Dict[str, Any]:
        """
        ðŸŽ¯ CREAZIONE ASSESSMENT ADATTIVO
        Dynamic assessment generation basato su profilo utente e skill target
        """
        try:
            # 1. USER PROFILING per assessment
            user_profile = await self._build_assessment_profile(user_id, skill_area)

            # 2. INITIAL DIFFICULTY ESTIMATION
            initial_difficulty = await self._estimate_initial_difficulty(user_profile, skill_area)

            # 3. QUESTION POOL GENERATION
            question_pool = await self._generate_question_pool(
                skill_area, initial_difficulty, target_duration_minutes
            )

            # 4. ADAPTIVE ALGORITHM SETUP
            adaptive_algorithm = await self._setup_adaptive_algorithm(
                user_profile, initial_difficulty, question_pool
            )

            # 5. ASSESSMENT STRUCTURE
            assessment_structure = {
                'assessment_id': f"adaptive_{user_id}_{datetime.now().timestamp()}",
                'user_id': user_id,
                'skill_area': skill_area,
                'type': assessment_type,
                'estimated_duration': target_duration_minutes,
                'initial_difficulty': initial_difficulty,
                'question_pool_size': len(question_pool),
                'adaptive_parameters': adaptive_algorithm,
                'created_at': datetime.now(),
                'status': 'ready'
            }

            await self.log_activity(
                f"Created adaptive assessment for user {user_id} in {skill_area}: "
                f"{len(question_pool)} questions, {initial_difficulty.value} starting difficulty"
            )

            return assessment_structure

        except Exception as e:
            await self.log_error(f"Adaptive assessment creation failed: {str(e)}")
            return {}

    async def administer_adaptive_test(
        self,
        assessment_id: str,
        user_responses: List[Dict[str, Any]]
    ) -> AdaptiveQuestion:
        """
        ðŸ”„ AMMINISTRAZIONE TEST ADATTIVO
        Real-time question selection basata su performance
        """
        try:
            # 1. RESPONSE ANALYSIS
            response_analysis = await self._analyze_user_responses(user_responses)

            # 2. ABILITY ESTIMATION UPDATE
            updated_ability = await self._update_ability_estimate(
                assessment_id, response_analysis
            )

            # 3. NEXT QUESTION SELECTION
            next_question = await self._select_next_question(
                assessment_id, updated_ability, response_analysis
            )

            # 4. STOPPING CRITERIA CHECK
            should_continue = await self._check_stopping_criteria(
                assessment_id, response_analysis, updated_ability
            )

            if not should_continue:
                # Test completion
                final_results = await self._finalize_assessment(
                    assessment_id, user_responses, updated_ability
                )
                return {'status': 'completed', 'results': final_results}

            # 5. QUESTION ADAPTATION
            adapted_question = await self._adapt_question_presentation(
                next_question, updated_ability, response_analysis
            )

            return adapted_question

        except Exception as e:
            await self.log_error(f"Adaptive test administration failed: {str(e)}")
            return None

    async def generate_intelligent_questions(
        self,
        topic: str,
        difficulty: DifficultyLevel,
        question_type: QuestionType,
        count: int = 5
    ) -> List[AdaptiveQuestion]:
        """
        ðŸ§  GENERAZIONE DOMANDE INTELLIGENTI
        AI-powered question creation con validazione automatica
        """
        try:
            questions = []

            for i in range(count):
                # 1. CONTENT ANALYSIS per topic
                content_analysis = await self._analyze_topic_content(topic)

                # 2. LEARNING OBJECTIVES EXTRACTION
                learning_objectives = await self._extract_learning_objectives(
                    topic, content_analysis
                )

                # 3. AI QUESTION GENERATION
                generated_question = await self._generate_ai_question(
                    topic, difficulty, question_type, learning_objectives
                )

                # 4. QUESTION VALIDATION
                validation_result = await self._validate_question_quality(
                    generated_question, topic, difficulty
                )

                if validation_result['valid']:
                    # 5. METADATA ENRICHMENT
                    enriched_question = await self._enrich_question_metadata(
                        generated_question, topic, difficulty, validation_result
                    )

                    questions.append(enriched_question)

            await self.log_activity(
                f"Generated {len(questions)} intelligent questions for {topic} "
                f"at {difficulty.value} level"
            )

            return questions

        except Exception as e:
            await self.log_error(f"Intelligent question generation failed: {str(e)}")
            return []

    async def evaluate_user_response(
        self,
        question: AdaptiveQuestion,
        user_response: str,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        âœ… VALUTAZIONE RISPOSTA UTENTE
        AI-powered response evaluation con feedback dettagliato
        """
        try:
            # 1. RESPONSE PREPROCESSING
            processed_response = await self._preprocess_user_response(
                user_response, question.question_type
            )

            # 2. ANSWER MATCHING/GRADING
            if question.question_type == QuestionType.MULTIPLE_CHOICE:
                evaluation = await self._evaluate_multiple_choice(
                    processed_response, question.correct_answer
                )
            elif question.question_type == QuestionType.CODE_CHALLENGE:
                evaluation = await self._evaluate_code_response(
                    processed_response, question, context
                )
            elif question.question_type in [QuestionType.SHORT_ANSWER, QuestionType.ESSAY]:
                evaluation = await self._evaluate_text_response(
                    processed_response, question, context
                )
            else:
                evaluation = await self._evaluate_generic_response(
                    processed_response, question, context
                )

            # 3. CONFIDENCE ASSESSMENT
            confidence_score = await self._assess_response_confidence(
                evaluation, question, context
            )

            # 4. DETAILED FEEDBACK GENERATION
            detailed_feedback = await self._generate_response_feedback(
                question, user_response, evaluation, confidence_score
            )

            # 5. LEARNING INSIGHTS
            learning_insights = await self._extract_learning_insights(
                question, evaluation, context
            )

            return {
                'question_id': question.id,
                'user_response': user_response,
                'is_correct': evaluation['correct'],
                'score': evaluation['score'],
                'confidence': confidence_score,
                'feedback': detailed_feedback,
                'explanation': question.explanation if not evaluation['correct'] else None,
                'learning_insights': learning_insights,
                'time_taken': context.get('time_taken', 0),
                'attempts': context.get('attempts', 1),
                'hint_used': context.get('hint_used', False)
            }

        except Exception as e:
            await self.log_error(f"Response evaluation failed: {str(e)}")
            return {'error': str(e)}

    async def assess_skill_proficiency(
        self,
        user_id: str,
        skill_name: str,
        evidence_data: List[Dict[str, Any]]
    ) -> SkillAssessment:
        """
        ðŸŽ¯ ASSESSMENT COMPETENZA SKILL
        Comprehensive skill proficiency evaluation con multiple evidence sources
        """
        try:
            # 1. EVIDENCE ANALYSIS
            evidence_analysis = await self._analyze_skill_evidence(
                evidence_data, skill_name
            )

            # 2. PROFICIENCY CALCULATION
            proficiency_metrics = await self._calculate_proficiency_metrics(
                evidence_analysis, skill_name
            )

            # 3. CONFIDENCE SCORING
            confidence_assessment = await self._assess_proficiency_confidence(
                evidence_analysis, proficiency_metrics
            )

            # 4. SUB-SKILL BREAKDOWN
            sub_skill_analysis = await self._analyze_sub_skills(
                evidence_analysis, skill_name
            )

            # 5. IMPROVEMENT RECOMMENDATIONS
            improvement_areas = await self._identify_improvement_areas(
                proficiency_metrics, sub_skill_analysis
            )

            # 6. VALIDATION METHODS
            validation_methods = await self._determine_validation_methods(
                evidence_analysis, proficiency_metrics
            )

            skill_assessment = SkillAssessment(
                skill_name=skill_name,
                proficiency_level=proficiency_metrics['level'],
                confidence_score=confidence_assessment['score'],
                evidence_strength=len(evidence_data),
                sub_skills=sub_skill_analysis,
                improvement_areas=improvement_areas,
                validation_methods=validation_methods,
                last_assessed=datetime.now()
            )

            await self.log_activity(
                f"Assessed skill proficiency for user {user_id} in {skill_name}: "
                f"{proficiency_metrics['level'].value} level"
            )

            return skill_assessment

        except Exception as e:
            await self.log_error(f"Skill proficiency assessment failed: {str(e)}")
            return None

    async def conduct_peer_assessment(
        self,
        assignment_id: str,
        reviewers: List[str],
        review_criteria: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ðŸ‘¥ CONDUZIONE PEER ASSESSMENT
        Orchestrated peer review con bias detection e quality control
        """
        try:
            # 1. REVIEWER ASSIGNMENT
            reviewer_assignments = await self._assign_peer_reviewers(
                assignment_id, reviewers, review_criteria
            )

            # 2. REVIEW GUIDELINES GENERATION
            review_guidelines = await self._generate_review_guidelines(
                assignment_id, review_criteria
            )

            # 3. BIAS DETECTION SETUP
            bias_detection = await self._setup_bias_detection(
                reviewers, review_criteria
            )

            # 4. QUALITY CONTROL MECHANISMS
            quality_controls = await self._setup_quality_controls(
                reviewer_assignments, review_criteria
            )

            # 5. PEER REVIEW ORCHESTRATION
            peer_review_structure = {
                'assignment_id': assignment_id,
                'reviewer_assignments': reviewer_assignments,
                'review_guidelines': review_guidelines,
                'bias_detection': bias_detection,
                'quality_controls': quality_controls,
                'deadline': datetime.now() + timedelta(days=7),
                'status': 'active',
                'created_at': datetime.now()
            }

            return peer_review_structure

        except Exception as e:
            await self.log_error(f"Peer assessment setup failed: {str(e)}")
            return {}

    async def _build_assessment_profile(self, user_id: str, skill_area: str) -> Dict:
        """ðŸ‘¤ Costruzione profilo assessment utente"""

        # Mock user assessment profile
        return {
            'user_id': user_id,
            'skill_area': skill_area,
            'prior_assessments': np.random.randint(0, 10),
            'average_score': np.random.uniform(0.4, 0.9),
            'learning_style': np.random.choice(['visual', 'auditory', 'kinesthetic']),
            'response_time_pattern': np.random.choice(['fast', 'moderate', 'deliberate']),
            'confidence_calibration': np.random.uniform(0.6, 1.0),
            'knowledge_areas': {
                f'{skill_area}_basics': np.random.uniform(0.3, 0.8),
                f'{skill_area}_intermediate': np.random.uniform(0.2, 0.7),
                f'{skill_area}_advanced': np.random.uniform(0.1, 0.6)
            }
        }

    async def _estimate_initial_difficulty(
        self,
        user_profile: Dict,
        skill_area: str
    ) -> DifficultyLevel:
        """ðŸŽ¯ Stima difficoltÃ  iniziale"""

        avg_score = user_profile['average_score']
        prior_assessments = user_profile['prior_assessments']

        # Adjust based on experience
        if prior_assessments == 0:
            # New user, start conservative
            if avg_score > 0.7:
                return DifficultyLevel.INTERMEDIATE
            else:
                return DifficultyLevel.BEGINNER
        else:
            # Experienced user
            if avg_score > 0.8:
                return DifficultyLevel.ADVANCED
            elif avg_score > 0.6:
                return DifficultyLevel.INTERMEDIATE
            else:
                return DifficultyLevel.BEGINNER

    async def _generate_question_pool(
        self,
        skill_area: str,
        initial_difficulty: DifficultyLevel,
        duration_minutes: int
    ) -> List[AdaptiveQuestion]:
        """ðŸ“š Generazione pool domande"""

        # Mock question pool generation
        pool_size = max(10, duration_minutes // 2)  # ~2 min per question
        questions = []

        for i in range(pool_size):
            # Vary difficulty around initial estimate
            if i < pool_size // 3:
                difficulty = DifficultyLevel.BEGINNER
            elif i < 2 * pool_size // 3:
                difficulty = DifficultyLevel.INTERMEDIATE
            else:
                difficulty = DifficultyLevel.ADVANCED

            question = AdaptiveQuestion(
                id=f"q_{skill_area}_{i}",
                question_text=f"Question about {skill_area} at {difficulty.value} level",
                question_type=QuestionType.MULTIPLE_CHOICE,
                difficulty_level=difficulty,
                skill_tags=[skill_area],
                options=["Option A", "Option B", "Option C", "Option D"],
                correct_answer="Option A",
                explanation=f"Explanation for {skill_area} concept",
                estimated_time_minutes=2,
                points=10,
                prerequisite_concepts=[],
                learning_objectives=[f"Understand {skill_area}"],
                cognitive_level="understand"
            )
            questions.append(question)

        return questions

    async def _setup_adaptive_algorithm(
        self,
        user_profile: Dict,
        initial_difficulty: DifficultyLevel,
        question_pool: List[AdaptiveQuestion]
    ) -> Dict:
        """âš™ï¸ Setup algoritmo adattivo"""

        return {
            'algorithm_type': 'item_response_theory',
            'ability_estimate': user_profile['average_score'],
            'ability_se': 0.3,  # Standard error
            'stopping_criteria': {
                'max_questions': len(question_pool),
                'min_questions': 5,
                'se_threshold': 0.2,
                'time_limit_minutes': 45
            },
            'selection_strategy': 'maximum_information',
            'difficulty_bounds': (0.1, 0.9)
        }

    async def _analyze_user_responses(
        self,
        user_responses: List[Dict[str, Any]]
    ) -> Dict:
        """ðŸ“Š Analisi risposte utente"""

        if not user_responses:
            return {'responses_count': 0, 'accuracy': 0.0, 'avg_time': 0.0}

        correct_count = sum(1 for r in user_responses if r.get('is_correct', False))
        total_time = sum(r.get('time_taken', 0) for r in user_responses)

        return {
            'responses_count': len(user_responses),
            'accuracy': correct_count / len(user_responses),
            'avg_time': total_time / len(user_responses),
            'difficulty_progression': [
                DifficultyLevel.BEGINNER if i < len(user_responses) // 2 
                else DifficultyLevel.INTERMEDIATE
                for i in range(len(user_responses))
            ],
            'response_patterns': {
                'guessing_detected': np.random.choice([True, False], p=[0.2, 0.8]),
                'fatigue_detected': np.random.choice([True, False], p=[0.3, 0.7])
            }
        }

    async def _analyze_topic_content(self, topic: str) -> Dict:
        """ðŸ” Analisi contenuto topic"""

        # AI-powered content analysis
        prompt = f"""
        Analizza il topic '{topic}' per la generazione di domande di assessment:

        Fornisci un'analisi strutturata che includa:
        1. Concetti chiave (3-5)
        2. Livelli di complessitÃ 
        3. Prerequisiti necessari
        4. Applicazioni pratiche
        5. Misconceptions comuni

        Formato JSON conciso.
        """

        try:
            response = await self.llm_service.generate_completion(
                prompt=prompt,
                max_tokens=300,
                temperature=0.3
            )

            # Simplified parsing
            return {
                'key_concepts': [f'{topic}_concept_{i}' for i in range(3)],
                'complexity_levels': ['basic', 'intermediate', 'advanced'],
                'prerequisites': [f'{topic}_prerequisite'],
                'applications': [f'{topic}_application_{i}' for i in range(2)],
                'misconceptions': [f'Common misconception about {topic}']
            }

        except Exception:
            return {
                'key_concepts': [topic],
                'complexity_levels': ['basic'],
                'prerequisites': [],
                'applications': [topic],
                'misconceptions': []
            }

    async def _generate_ai_question(
        self,
        topic: str,
        difficulty: DifficultyLevel,
        question_type: QuestionType,
        learning_objectives: List[str]
    ) -> Dict:
        """ðŸ¤– Generazione domanda AI"""

        # AI prompt for question generation
        prompt = f"""
        Genera una domanda di assessment per:

        Topic: {topic}
        DifficoltÃ : {difficulty.value}
        Tipo: {question_type.value}
        Obiettivi: {', '.join(learning_objectives)}

        Genera:
        1. Testo domanda chiaro e specifico
        2. Opzioni di risposta (se applicabile)
        3. Risposta corretta
        4. Spiegazione dettagliata

        Assicurati che la domanda sia:
        - Appropriata per il livello di difficoltÃ 
        - Allineata agli obiettivi di apprendimento
        - Priva di ambiguitÃ 
        """

        try:
            response = await self.llm_service.generate_completion(
                prompt=prompt,
                max_tokens=400,
                temperature=0.7
            )

            # Parse AI response (simplified)
            return {
                'question_text': f"What is the main concept of {topic}?",
                'options': ["Option A", "Option B", "Option C", "Option D"] if question_type == QuestionType.MULTIPLE_CHOICE else None,
                'correct_answer': "Option A",
                'explanation': f"The correct answer explains the core concept of {topic}."
            }

        except Exception:
            return {
                'question_text': f"Explain {topic}",
                'options': None,
                'correct_answer': f"General answer about {topic}",
                'explanation': f"Basic explanation of {topic}"
            }

    async def _validate_question_quality(
        self,
        question: Dict,
        topic: str,
        difficulty: DifficultyLevel
    ) -> Dict:
        """âœ… Validazione qualitÃ  domanda"""

        # Quality validation metrics
        validation_checks = {
            'clarity_score': np.random.uniform(0.7, 1.0),
            'difficulty_alignment': np.random.uniform(0.8, 1.0),
            'content_accuracy': np.random.uniform(0.9, 1.0),
            'bias_free': np.random.choice([True, False], p=[0.9, 0.1]),
            'answer_quality': np.random.uniform(0.8, 1.0)
        }

        overall_score = np.mean(list(validation_checks.values())[:-1])  # Exclude boolean

        return {
            'valid': overall_score > 0.7 and validation_checks['bias_free'],
            'quality_score': overall_score,
            'validation_checks': validation_checks,
            'improvement_suggestions': [] if overall_score > 0.8 else ['Improve clarity', 'Check difficulty alignment']
        }

    async def _evaluate_code_response(
        self,
        code_response: str,
        question: AdaptiveQuestion,
        context: Dict
    ) -> Dict:
        """ðŸ’» Valutazione risposta codice"""

        # Mock code evaluation (in produzione userebbe code execution sandbox)
        try:
            # Syntax check simulation
            syntax_valid = len(code_response.strip()) > 10 and 'def ' in code_response

            # Logic check simulation
            logic_score = np.random.uniform(0.3, 1.0) if syntax_valid else 0.0

            # Style check simulation  
            style_score = np.random.uniform(0.6, 1.0) if syntax_valid else 0.0

            # Efficiency check simulation
            efficiency_score = np.random.uniform(0.5, 1.0) if logic_score > 0.7 else 0.3

            overall_score = np.mean([logic_score, style_score, efficiency_score]) if syntax_valid else 0.0

            return {
                'correct': overall_score > 0.6,
                'score': overall_score,
                'breakdown': {
                    'syntax': syntax_valid,
                    'logic': logic_score,
                    'style': style_score,
                    'efficiency': efficiency_score
                },
                'feedback_points': [
                    'Code syntax is correct' if syntax_valid else 'Syntax errors detected',
                    f'Logic implementation: {logic_score:.1%}',
                    f'Code style: {style_score:.1%}'
                ]
            }

        except Exception as e:
            return {
                'correct': False,
                'score': 0.0,
                'error': str(e),
                'feedback_points': ['Code evaluation failed']
            }

    async def _evaluate_text_response(
        self,
        text_response: str,
        question: AdaptiveQuestion,
        context: Dict
    ) -> Dict:
        """ðŸ“ Valutazione risposta testuale"""

        # AI-powered text evaluation
        prompt = f"""
        Valuta questa risposta testuale:

        Domanda: {question.question_text}
        Risposta studente: {text_response}
        Risposta corretta: {question.correct_answer}

        Valuta su scala 0-1:
        1. Accuratezza del contenuto
        2. Completezza della risposta
        3. Chiarezza espositiva
        4. Uso di esempi/evidenze

        Fornisci score totale e feedback costruttivo.
        """

        try:
            response = await self.llm_service.generate_completion(
                prompt=prompt,
                max_tokens=200,
                temperature=0.3
            )

            # Parse AI evaluation (simplified)
            score = np.random.uniform(0.4, 0.95)

            return {
                'correct': score > 0.6,
                'score': score,
                'breakdown': {
                    'accuracy': np.random.uniform(0.5, 1.0),
                    'completeness': np.random.uniform(0.4, 0.9),
                    'clarity': np.random.uniform(0.6, 1.0),
                    'evidence': np.random.uniform(0.3, 0.8)
                },
                'ai_feedback': response.strip()
            }

        except Exception:
            # Fallback simple evaluation
            word_count = len(text_response.split())
            score = min(1.0, word_count / 50)  # Rough scoring

            return {
                'correct': score > 0.5,
                'score': score,
                'feedback_points': [
                    f'Response length: {word_count} words',
                    'Basic content evaluation applied'
                ]
            }

    async def _generate_response_feedback(
        self,
        question: AdaptiveQuestion,
        user_response: str,
        evaluation: Dict,
        confidence: float
    ) -> str:
        """ðŸ’¬ Generazione feedback risposta"""

        # AI-powered feedback generation
        feedback_prompt = f"""
        Genera feedback costruttivo per questa risposta:

        Domanda: {question.question_text}
        Risposta studente: {user_response}
        Valutazione: {evaluation['score']:.1%}
        Confidence: {confidence:.1%}

        Crea feedback che:
        1. Riconosca gli aspetti positivi
        2. Identifichi aree di miglioramento
        3. Suggerisca passi concreti
        4. Mantenga tono incoraggiante

        Max 100 parole.
        """

        try:
            feedback = await self.llm_service.generate_completion(
                prompt=feedback_prompt,
                max_tokens=150,
                temperature=0.7
            )
            return feedback.strip()

        except Exception:
            # Fallback feedback
            if evaluation['score'] > 0.8:
                return "Ottima risposta! Hai dimostrato una solida comprensione del concetto."
            elif evaluation['score'] > 0.6:
                return "Buona risposta! Alcuni aspetti potrebbero essere approfonditi."
            else:
                return "La risposta mostra comprensione parziale. Ti consiglio di rivedere i concetti base."

    # Metodi helper aggiuntivi...
    async def _preprocess_user_response(self, response: str, question_type: QuestionType) -> str:
        """ðŸ”„ Preprocessing risposta utente"""
        if not response:
            return ""

        # Basic preprocessing
        cleaned = response.strip()

        if question_type == QuestionType.CODE_CHALLENGE:
            # Remove common code formatting issues
            cleaned = cleaned.replace('\n', '
').replace('\t', '	')

        return cleaned

    async def _evaluate_multiple_choice(self, user_answer: str, correct_answer: str) -> Dict:
        """âœ… Valutazione multiple choice"""
        is_correct = user_answer.strip().lower() == correct_answer.strip().lower()
        return {
            'correct': is_correct,
            'score': 1.0 if is_correct else 0.0,
            'feedback_points': ['Correct answer!' if is_correct else 'Incorrect selection']
        }

    async def _evaluate_generic_response(
        self, 
        response: str, 
        question: AdaptiveQuestion, 
        context: Dict
    ) -> Dict:
        """ðŸ“‹ Valutazione generica"""
        # Fallback evaluation
        has_content = len(response.strip()) > 0
        return {
            'correct': has_content,
            'score': 0.5 if has_content else 0.0,
            'feedback_points': ['Response provided' if has_content else 'No response given']
        }

    async def _extract_learning_objectives(self, topic: str, content_analysis: Dict) -> List[str]:
        """ðŸŽ¯ Estrazione obiettivi apprendimento"""
        return [
            f"Define key concepts of {topic}",
            f"Apply {topic} principles",
            f"Analyze {topic} scenarios"
        ]

    async def _enrich_question_metadata(
        self, 
        question: Dict, 
        topic: str, 
        difficulty: DifficultyLevel, 
        validation: Dict
    ) -> AdaptiveQuestion:
        """ðŸ“Š Arricchimento metadata domanda"""
        return AdaptiveQuestion(
            id=f"generated_{topic}_{int(datetime.now().timestamp())}",
            question_text=question['question_text'],
            question_type=QuestionType.MULTIPLE_CHOICE,
            difficulty_level=difficulty,
            skill_tags=[topic],
            options=question.get('options'),
            correct_answer=question['correct_answer'],
            explanation=question['explanation'],
            estimated_time_minutes=2,
            points=10,
            prerequisite_concepts=[],
            learning_objectives=[f"Understand {topic}"],
            cognitive_level="understand"
        )

    # Metodi per skill assessment, peer review, etc. (simplified versions)
    async def _analyze_skill_evidence(self, evidence_data: List[Dict], skill: str) -> Dict:
        """ðŸ“Š Analisi evidenze skill"""
        return {
            'evidence_count': len(evidence_data),
            'avg_performance': np.random.uniform(0.5, 0.9),
            'consistency': np.random.uniform(0.6, 0.95),
            'evidence_types': ['assessment', 'project', 'practice']
        }

    async def _calculate_proficiency_metrics(self, evidence: Dict, skill: str) -> Dict:
        """ðŸ“ˆ Calcolo metriche competenza"""
        performance = evidence['avg_performance']

        if performance > 0.8:
            level = DifficultyLevel.ADVANCED
        elif performance > 0.6:
            level = DifficultyLevel.INTERMEDIATE
        else:
            level = DifficultyLevel.BEGINNER

        return {
            'level': level,
            'score': performance,
            'growth_rate': np.random.uniform(0.05, 0.2)
        }

    # Altri metodi helper semplificati...
    async def _update_ability_estimate(self, assessment_id: str, analysis: Dict) -> float:
        return min(1.0, max(0.0, analysis['accuracy']))

    async def _select_next_question(self, assessment_id: str, ability: float, analysis: Dict) -> AdaptiveQuestion:
        # Mock next question selection
        return AdaptiveQuestion(
            id="next_q", question_text="Next adaptive question", 
            question_type=QuestionType.MULTIPLE_CHOICE,
            difficulty_level=DifficultyLevel.INTERMEDIATE,
            skill_tags=["general"], options=["A", "B", "C"], 
            correct_answer="A", explanation="Explanation",
            estimated_time_minutes=2, points=10,
            prerequisite_concepts=[], learning_objectives=["Learn"],
            cognitive_level="understand"
        )

    async def _check_stopping_criteria(self, assessment_id: str, analysis: Dict, ability: float) -> bool:
        return analysis['responses_count'] < 10  # Continue if less than 10 questions

    async def _finalize_assessment(self, assessment_id: str, responses: List, ability: float) -> AssessmentResult:
        return AssessmentResult(
            user_id="user", assessment_id=assessment_id, overall_score=ability,
            skill_scores={"general": ability}, time_taken_minutes=30,
            questions_attempted=len(responses), questions_correct=int(len(responses) * ability),
            difficulty_progression=[DifficultyLevel.INTERMEDIATE],
            knowledge_gaps=["advanced_concepts"], mastery_areas=["basics"],
            recommendations=["Continue practicing"], confidence_intervals={"general": (ability-0.1, ability+0.1)},
            next_learning_steps=["Next topic"], certification_eligible=ability > 0.8,
            detailed_feedback="Good progress overall"
        )
